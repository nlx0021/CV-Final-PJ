Model:

Model: ViT(
  (to_patch_embedding): Sequential(
    (0): Rearrange('b c (h p1) (w p2) -> b (h w) (p1 p2 c)', p1=8, p2=8)
    (1): Linear(in_features=192, out_features=512, bias=True)
  )
  (dropout): Dropout(p=0, inplace=False)
  (transformer): Transformer(
    (layers): ModuleList(
      (0): ModuleList(
        (0): PreNorm(
          (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (fn): Attention(
            (attend): Softmax(dim=-1)
            (dropout): Dropout(p=0, inplace=False)
            (to_qkv): Linear(in_features=512, out_features=1536, bias=False)
            (to_out): Sequential(
              (0): Linear(in_features=512, out_features=512, bias=True)
              (1): Dropout(p=0, inplace=False)
            )
          )
        )
        (1): PreNorm(
          (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (fn): FeedForward(
            (net): Sequential(
              (0): Linear(in_features=512, out_features=512, bias=True)
              (1): GELU()
              (2): Dropout(p=0, inplace=False)
              (3): Linear(in_features=512, out_features=512, bias=True)
              (4): Dropout(p=0, inplace=False)
            )
          )
        )
      )
      (1): ModuleList(
        (0): PreNorm(
          (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (fn): Attention(
            (attend): Softmax(dim=-1)
            (dropout): Dropout(p=0, inplace=False)
            (to_qkv): Linear(in_features=512, out_features=1536, bias=False)
            (to_out): Sequential(
              (0): Linear(in_features=512, out_features=512, bias=True)
              (1): Dropout(p=0, inplace=False)
            )
          )
        )
        (1): PreNorm(
          (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (fn): FeedForward(
            (net): Sequential(
              (0): Linear(in_features=512, out_features=512, bias=True)
              (1): GELU()
              (2): Dropout(p=0, inplace=False)
              (3): Linear(in_features=512, out_features=512, bias=True)
              (4): Dropout(p=0, inplace=False)
            )
          )
        )
      )
      (2): ModuleList(
        (0): PreNorm(
          (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (fn): Attention(
            (attend): Softmax(dim=-1)
            (dropout): Dropout(p=0, inplace=False)
            (to_qkv): Linear(in_features=512, out_features=1536, bias=False)
            (to_out): Sequential(
              (0): Linear(in_features=512, out_features=512, bias=True)
              (1): Dropout(p=0, inplace=False)
            )
          )
        )
        (1): PreNorm(
          (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (fn): FeedForward(
            (net): Sequential(
              (0): Linear(in_features=512, out_features=512, bias=True)
              (1): GELU()
              (2): Dropout(p=0, inplace=False)
              (3): Linear(in_features=512, out_features=512, bias=True)
              (4): Dropout(p=0, inplace=False)
            )
          )
        )
      )
      (3): ModuleList(
        (0): PreNorm(
          (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (fn): Attention(
            (attend): Softmax(dim=-1)
            (dropout): Dropout(p=0, inplace=False)
            (to_qkv): Linear(in_features=512, out_features=1536, bias=False)
            (to_out): Sequential(
              (0): Linear(in_features=512, out_features=512, bias=True)
              (1): Dropout(p=0, inplace=False)
            )
          )
        )
        (1): PreNorm(
          (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (fn): FeedForward(
            (net): Sequential(
              (0): Linear(in_features=512, out_features=512, bias=True)
              (1): GELU()
              (2): Dropout(p=0, inplace=False)
              (3): Linear(in_features=512, out_features=512, bias=True)
              (4): Dropout(p=0, inplace=False)
            )
          )
        )
      )
      (4): ModuleList(
        (0): PreNorm(
          (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (fn): Attention(
            (attend): Softmax(dim=-1)
            (dropout): Dropout(p=0, inplace=False)
            (to_qkv): Linear(in_features=512, out_features=1536, bias=False)
            (to_out): Sequential(
              (0): Linear(in_features=512, out_features=512, bias=True)
              (1): Dropout(p=0, inplace=False)
            )
          )
        )
        (1): PreNorm(
          (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (fn): FeedForward(
            (net): Sequential(
              (0): Linear(in_features=512, out_features=512, bias=True)
              (1): GELU()
              (2): Dropout(p=0, inplace=False)
              (3): Linear(in_features=512, out_features=512, bias=True)
              (4): Dropout(p=0, inplace=False)
            )
          )
        )
      )
      (5): ModuleList(
        (0): PreNorm(
          (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (fn): Attention(
            (attend): Softmax(dim=-1)
            (dropout): Dropout(p=0, inplace=False)
            (to_qkv): Linear(in_features=512, out_features=1536, bias=False)
            (to_out): Sequential(
              (0): Linear(in_features=512, out_features=512, bias=True)
              (1): Dropout(p=0, inplace=False)
            )
          )
        )
        (1): PreNorm(
          (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (fn): FeedForward(
            (net): Sequential(
              (0): Linear(in_features=512, out_features=512, bias=True)
              (1): GELU()
              (2): Dropout(p=0, inplace=False)
              (3): Linear(in_features=512, out_features=512, bias=True)
              (4): Dropout(p=0, inplace=False)
            )
          )
        )
      )
      (6): ModuleList(
        (0): PreNorm(
          (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (fn): Attention(
            (attend): Softmax(dim=-1)
            (dropout): Dropout(p=0, inplace=False)
            (to_qkv): Linear(in_features=512, out_features=1536, bias=False)
            (to_out): Sequential(
              (0): Linear(in_features=512, out_features=512, bias=True)
              (1): Dropout(p=0, inplace=False)
            )
          )
        )
        (1): PreNorm(
          (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (fn): FeedForward(
            (net): Sequential(
              (0): Linear(in_features=512, out_features=512, bias=True)
              (1): GELU()
              (2): Dropout(p=0, inplace=False)
              (3): Linear(in_features=512, out_features=512, bias=True)
              (4): Dropout(p=0, inplace=False)
            )
          )
        )
      )
    )
  )
  (to_latent): Identity()
  (mlp_head): Sequential(
    (0): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    (1): Linear(in_features=512, out_features=100, bias=True)
  )
)
Parameters:

Learning rate: 0.020000
Total epoches: 200
Loss function: CrossEntropyLoss()
Curves:

learning_curve: [4.197766532952135, 3.81314321200956, 3.6374328102577818, 3.5187440236861054, 3.4542739005251364, 3.4357375902208416, 3.3093541528690946, 3.3141432113268157, 3.291719654066996, 3.2815875045277854, 3.302193037488244, 3.2021362287077038, 3.2428655197674576, 3.1022491509264167, 3.154937880283052, 3.106935765932907, 3.1104358515956183, 3.1116905124350027, 3.111585308204998, 3.034547903321006, 3.0766847370700403, 3.0396477857773956, 2.993197543377226, 2.9266252534633335, 2.9357448464090172, 2.99007003957575, 2.958710940385407, 2.901785127141259, 2.8671810914846985, 2.903179507363926, 2.922879435122013, 2.8647587702355604, 2.928526345979084, 2.902495217932896, 2.863933275030418, 2.8192519846964967, 2.815227930518714, 2.8107432276010513, 2.78460216420618, 2.8466343239627103, 2.736553073268045, 2.8775123577903616, 2.7599143995480104, 2.7351670884950594, 2.735513993284919, 2.8201966864818875, 2.7624192278493536, 2.810502104461193, 2.7613038322464987, 2.71118225292726, 2.5146816735240547, 2.4274502494795756, 2.4272751669314774, 2.4296704679727554, 2.4295915368605745, 2.471572547135028, 2.495355839756402, 2.360064247792417, 2.4555418145927517, 2.497000677003102, 2.449895889582959, 2.4656482159414073, 2.4382502761754123, 2.371560414745049, 2.341157094998793, 2.3990992338142614, 2.3737376352602784, 2.346531272273172, 2.3269298513504593, 2.319148417900909, 2.438813018527898, 2.467963255603205, 2.3744485080242157, 2.3397672545503485, 2.3581485724584623, 2.3326124874028293, 2.27638029307127, 2.371871500868689, 2.2876739485019986, 2.2975441549311983, 2.343201818791303, 2.380541367625648, 2.3008886602791874, 2.285198398611762, 2.312649583274668, 2.2727559374814685, 2.25534592772072, 2.2169732092456385, 2.2558821626007557, 2.3199383911084044, 2.063356484879147, 1.9625450252470644, 2.0954671290449123, 1.9972571998157285, 1.955110095780004, 2.0524087737907064, 1.9765030628239566, 2.128101433203979, 2.005648894743486, 2.021409251981161, 1.8926680057563565, 1.9222188867967238, 1.925275095315142, 2.0485371735624294, 2.038915816525167, 2.0690622505816547, 1.9365509130399337, 1.9080355495891788, 2.012309814887968, 1.9709549446336248, 2.0104303620755672, 2.0459019140425054, 1.8682296013628894, 2.0026070057329806, 2.0609382038766686, 1.9094935496422378, 1.8935201733627103, 1.8601911542090503, 1.8238143622875214, 1.782136375423182, 1.861914698542519, 1.9436660629104485, 1.807754400101575, 1.87402542934499, 1.7879341249777512, 1.9158370200205932, 1.6982794627547264, 1.698049583556977, 1.7597869601608678, 1.8478484421291135, 1.6943023214963349, 1.7274684265933253, 1.8128686095164581, 1.7838582279668613, 1.6813827881758863, 1.8050760076465933, 1.781173482706601, 1.7434859790585258, 1.7030352103439244, 1.803044240583073, 1.7852349310097368, 1.7336789745498786, 1.7031349875032902, 1.7114793726835738, 1.7284969204867429, 1.599352878687734, 1.8267908863384614, 1.815950465981256, 1.7633762191981077, 1.6823764276606115, 1.618322126228701, 1.575523018074984, 1.6540101060474461, 1.6959632413292474, 1.5787187608128244, 1.8325412221760913, 1.6523917134512554, 1.72441167130389, 1.639184494587508, 1.8562979157167403, 1.7073219874873757, 1.6443649347532878, 1.7109168951653622, 1.658679884435101, 1.7388291710310362, 1.823336904445155, 1.769124086438255, 1.7297888982363723, 1.7071950428018516, 1.7112037847665222, 1.7390770290724256, 1.557241770692847, 1.7449583977630192, 1.6253240046684037, 1.645485196855258, 1.6311504967849364, 1.6451336341825398, 1.6456101342019709, 1.6381308134983887, 1.7131667535921389, 1.5062376474114982, 1.6439584093168378, 1.6234436797994105, 1.7361101466654376, 1.8334942112930797, 1.7463763338429006, 1.696165922859853, 1.6949981627985835, 1.6628909870474176, 1.6007773653519424, 1.6121065856211565, 1.589456973682073, 1.7983252465386281, 1.7361035147173838, 1.7494832725179466, 1.656006800942123, 1.6936703269285234, 1.6912768754938787, 1.748076300729405, 1.657358998856084]
val_loss_curve: [3.586821573972702, 3.3448012709617614, 3.2266536235809324, 3.106744408607483, 3.036028951406479, 2.9828747153282165, 2.90083846449852, 2.890794038772583, 2.882637304067612, 2.7922022104263307, 2.8417112827301025, 2.74445121884346, 2.7570346534252166, 2.6813642561435698, 2.7050057113170625, 2.7221662163734437, 2.765380209684372, 2.669781321287155, 2.6625520169734953, 2.6116727232933044, 2.562369465827942, 2.569262421131134, 2.589422422647476, 2.591632241010666, 2.495469844341278, 2.499362939596176, 2.537847238779068, 2.4600692451000215, 2.4384137988090515, 2.5168025195598602, 2.4130724340677263, 2.416852241754532, 2.451829397678375, 2.4081093043088915, 2.4563336431980134, 2.389168179035187, 2.362353813648224, 2.401022899150848, 2.3477607131004334, 2.4206359505653383, 2.308573415875435, 2.3780515253543855, 2.367598223686218, 2.3223451137542725, 2.316523540019989, 2.2910832911729813, 2.3231542825698854, 2.3471888482570646, 2.3604384899139403, 2.2771975666284563, 2.108492758870125, 2.0996091246604918, 2.101439818739891, 2.092657658457756, 2.1304341435432432, 2.082437017560005, 2.074156478047371, 2.0887740939855575, 2.09220304787159, 2.056488361954689, 2.1162335842847826, 2.093036761879921, 2.1201072305440904, 2.082135355472565, 2.1330154657363893, 2.1397154569625854, 2.0795177578926087, 2.0904505133628843, 2.0939466625452043, 2.119168347120285, 2.1476094871759415, 2.084578570723534, 2.0519586354494095, 2.121625933051109, 2.1544486612081526, 2.0940470516681673, 2.0501434803009033, 2.059828442335129, 2.09564471244812, 2.0522207260131835, 2.0416863024234773, 2.0208189308643343, 2.087894657254219, 2.046682831645012, 2.0691074997186663, 2.1464680969715118, 2.082699951529503, 2.0668651461601257, 2.096074876189232, 2.068204638361931, 1.9402028560638427, 1.9599738776683808, 1.9371412754058839, 1.9561347723007203, 1.9522333294153214, 1.9523421794176101, 1.941109400987625, 1.9905932426452637, 1.978728500008583, 2.0072368294000626, 1.9689411789178848, 1.9515964478254317, 1.974037954211235, 1.922905969619751, 1.965580552816391, 1.954696360230446, 1.9613149523735047, 1.9910491704940796, 1.98123379945755, 1.9859903812408448, 1.9356019109487534, 1.9838652342557908, 1.9510208159685134, 1.9851013332605363, 1.9794021189212798, 1.9799913614988327, 1.968152928352356, 2.0127671122550965, 1.9458520263433456, 2.007048749923706, 1.979152262210846, 1.9569531708955765, 1.979719939827919, 1.9291591197252274, 1.953247883915901, 1.9747558504343032, 1.9283925354480744, 1.9344468057155608, 1.968204003572464, 1.9244368553161622, 1.9248896926641463, 1.937155818939209, 1.9448952496051788, 1.9470806390047073, 1.9661698013544082, 1.9300573244690895, 1.9424424350261689, 1.936924621462822, 1.9926850974559784, 2.0056560814380644, 1.9705461531877517, 1.9547467172145843, 1.935979315638542, 1.9703096270561218, 1.9367927372455598, 1.9871795892715454, 1.9627889007329942, 1.9557017028331756, 1.961950844526291, 2.021875897049904, 1.9655583918094635, 1.9979864448308944, 1.936909168958664, 1.9302081376314164, 1.9350724011659621, 1.950466525554657, 1.9530620008707047, 1.961960604786873, 1.960927626490593, 1.9562300801277162, 1.9528596132993699, 1.9458552300930023, 1.9709128260612487, 1.9469811737537384, 1.9419418692588806, 1.9481633305549622, 1.9876985043287276, 1.9941506296396256, 1.9553581297397613, 1.9628741919994355, 1.9802194327116012, 1.9836344361305236, 1.923642709851265, 1.9448668390512467, 1.9578828066587448, 1.959281262755394, 1.934471882879734, 1.9526261031627654, 1.947474130988121, 2.000048589706421, 1.9671249449253083, 1.96376693546772, 1.9657129615545272, 1.9744422435760498, 1.962203285098076, 1.9467613607645036, 1.9658167153596877, 2.006032791733742, 1.955467027425766, 1.9372407019138336, 1.975057590007782, 1.9621110022068025, 1.9411984741687776, 1.9552367508411408, 1.924952694773674, 1.952527615427971, 1.9394506990909577, 1.9454049676656724, 1.9511002719402313, 1.9668512254953385]
train_accuracy_curve[0.08066666666666666, 0.1354, 0.16557777777777777, 0.18555555555555556, 0.19564444444444445, 0.20746666666666666, 0.2257111111111111, 0.22746666666666668, 0.2323111111111111, 0.23413333333333333, 0.23477777777777778, 0.2531777777777778, 0.24264444444444444, 0.27155555555555555, 0.25953333333333334, 0.2708, 0.2691777777777778, 0.2713333333333333, 0.2693111111111111, 0.2869333333333333, 0.27828888888888886, 0.2854888888888889, 0.2952222222222222, 0.31133333333333335, 0.3051111111111111, 0.2956666666666667, 0.3027111111111111, 0.31453333333333333, 0.32251111111111114, 0.31693333333333334, 0.3154222222222222, 0.31975555555555557, 0.3157111111111111, 0.3144888888888889, 0.32053333333333334, 0.3338, 0.3323111111111111, 0.33715555555555554, 0.34157777777777776, 0.3294, 0.3526888888888889, 0.32815555555555553, 0.34704444444444443, 0.35102222222222224, 0.3524, 0.3372888888888889, 0.34175555555555553, 0.3410888888888889, 0.3440888888888889, 0.3589111111111111, 0.4064, 0.42577777777777776, 0.42602222222222225, 0.42475555555555555, 0.4270888888888889, 0.41906666666666664, 0.41417777777777776, 0.4444444444444444, 0.42277777777777775, 0.4159555555555556, 0.4204888888888889, 0.4242888888888889, 0.43291111111111114, 0.4444222222222222, 0.44935555555555556, 0.4355777777777778, 0.44251111111111113, 0.44477777777777777, 0.4462, 0.45611111111111113, 0.4250888888888889, 0.4325777777777778, 0.4397333333333333, 0.45008888888888887, 0.4449777777777778, 0.4487111111111111, 0.4684, 0.4453111111111111, 0.46453333333333335, 0.45644444444444443, 0.45955555555555555, 0.43944444444444447, 0.4558, 0.46673333333333333, 0.4580888888888889, 0.4661777777777778, 0.4700888888888889, 0.4806666666666667, 0.4692222222222222, 0.4549111111111111, 0.5309777777777778, 0.5454666666666667, 0.5190222222222223, 0.5463333333333333, 0.5548222222222222, 0.5357555555555555, 0.5457111111111111, 0.5129555555555556, 0.5465555555555556, 0.5440666666666667, 0.5644444444444444, 0.5601555555555555, 0.5556444444444445, 0.5339333333333334, 0.537, 0.5260222222222222, 0.5591111111111111, 0.564, 0.5447333333333333, 0.5556444444444445, 0.5519555555555555, 0.5335111111111112, 0.5765111111111111, 0.5471555555555555, 0.5404666666666667, 0.5661333333333334, 0.5679333333333333, 0.5768222222222222, 0.5883555555555555, 0.5914444444444444, 0.5834222222222222, 0.5755333333333333, 0.5998666666666667, 0.5908222222222222, 0.6025777777777778, 0.5902666666666667, 0.6216, 0.6297111111111111, 0.6172888888888889, 0.5978666666666667, 0.6378666666666667, 0.6255111111111111, 0.604, 0.6111111111111112, 0.6333555555555556, 0.6007777777777777, 0.6140222222222222, 0.6243555555555556, 0.6313333333333333, 0.6089333333333333, 0.6037111111111111, 0.6227777777777778, 0.6336444444444445, 0.6313555555555556, 0.6258888888888889, 0.6480444444444444, 0.6064666666666667, 0.6026, 0.6186666666666667, 0.6422, 0.65, 0.6606888888888889, 0.6491555555555556, 0.6333555555555556, 0.6599111111111111, 0.6016888888888889, 0.6457777777777778, 0.6252444444444445, 0.6495555555555556, 0.6104, 0.6401333333333333, 0.6487333333333334, 0.6306444444444445, 0.642, 0.628, 0.6008444444444444, 0.6324888888888889, 0.6283555555555556, 0.6400444444444444, 0.6401777777777777, 0.6321555555555556, 0.6729333333333334, 0.6297555555555555, 0.6556222222222222, 0.6490222222222222, 0.6558222222222222, 0.6448, 0.6511777777777777, 0.6544888888888889, 0.6305777777777778, 0.6843333333333333, 0.6501333333333333, 0.6536, 0.6383111111111112, 0.6123777777777778, 0.6325333333333333, 0.6452, 0.6404666666666666, 0.6593555555555556, 0.6588888888888889, 0.6707333333333333, 0.6603111111111111, 0.6182222222222222, 0.6323333333333333, 0.6244888888888889, 0.6494888888888889, 0.6557333333333333, 0.6452666666666667, 0.6312222222222222, 0.6499333333333334]
val_accuracy_curve[0.1536, 0.2, 0.2202, 0.243, 0.2616, 0.2684, 0.2832, 0.286, 0.3016, 0.3094, 0.299, 0.319, 0.3228, 0.3282, 0.3226, 0.3178, 0.3198, 0.3328, 0.3398, 0.335, 0.3538, 0.3568, 0.354, 0.343, 0.3704, 0.3684, 0.3622, 0.3804, 0.3782, 0.373, 0.3846, 0.3894, 0.377, 0.394, 0.3784, 0.392, 0.3906, 0.3946, 0.4038, 0.3992, 0.412, 0.396, 0.399, 0.4114, 0.4114, 0.4142, 0.4112, 0.3992, 0.4034, 0.419, 0.4554, 0.4526, 0.4578, 0.4546, 0.4492, 0.462, 0.4612, 0.4548, 0.4534, 0.463, 0.455, 0.4566, 0.4562, 0.4562, 0.4512, 0.444, 0.457, 0.452, 0.4548, 0.454, 0.4462, 0.4526, 0.4634, 0.4572, 0.4508, 0.4572, 0.4596, 0.4678, 0.4716, 0.4656, 0.4702, 0.4608, 0.464, 0.4664, 0.4646, 0.4566, 0.4626, 0.465, 0.4654, 0.4674, 0.4942, 0.4934, 0.4924, 0.4948, 0.4964, 0.4988, 0.4976, 0.4994, 0.496, 0.4956, 0.4944, 0.4972, 0.4992, 0.4952, 0.4942, 0.496, 0.4964, 0.4952, 0.496, 0.4912, 0.5008, 0.4978, 0.5014, 0.4922, 0.4924, 0.4974, 0.4978, 0.4882, 0.4994, 0.483, 0.5026, 0.5072, 0.496, 0.51, 0.51, 0.506, 0.5114, 0.5044, 0.5046, 0.5084, 0.5086, 0.5068, 0.5116, 0.5084, 0.5106, 0.5088, 0.5102, 0.5116, 0.5042, 0.5046, 0.5024, 0.5082, 0.5192, 0.509, 0.5104, 0.5088, 0.5084, 0.5078, 0.5072, 0.5022, 0.5122, 0.5112, 0.5112, 0.509, 0.514, 0.5106, 0.514, 0.5118, 0.5132, 0.509, 0.5106, 0.5106, 0.5132, 0.509, 0.515, 0.5162, 0.5102, 0.5134, 0.5124, 0.5118, 0.51, 0.5096, 0.5132, 0.5116, 0.5086, 0.5126, 0.511, 0.51, 0.5128, 0.5116, 0.517, 0.5132, 0.5156, 0.5158, 0.5148, 0.5134, 0.514, 0.512, 0.5124, 0.5146, 0.5126, 0.5108, 0.514, 0.513, 0.5126, 0.5128, 0.5142, 0.5136, 0.5154, 0.5136]
max_val_accuracy: 0.519200
max_val_accuracy_epoch: 142
Test result:

Test accuracy: 0.515700
The number of para:

para_n: 11195492
